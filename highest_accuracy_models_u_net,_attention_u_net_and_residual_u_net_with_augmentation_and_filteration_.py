# -*- coding: utf-8 -*-
"""Highest Accuracy models: U-Net, Attention U-Net and Residual U-Net with Augmentation and Filteration .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5bQ9s-JX4r7aZUv2Lqkn1_stL3FyUDr

Using API Key to import dataset from Kaggle
"""

!pip install imantics --quiet
import zipfile
import os
!pip install -q kaggle

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d maedemaftouni/covid19-ct-scan-lesion-segmentation-dataset

# Extract the dataset
!unzip -q covid19-ct-scan-lesion-segmentation-dataset.zip -d /content/CT_Dataset/

# Check extracted contents
!ls /content/CT_Dataset/

os.listdir()

"""Install Dependencies"""

import os #creating folders, managing file-paths, creating directories, reading images from folders
import numpy as np
import cv2
import random
import matplotlib.pyplot as plt
import shutil #For deleting empty masks
from glob import glob
from sklearn.model_selection import train_test_split
from albumentations import HorizontalFlip, VerticalFlip, Rotate #effecient data augmentation
from tqdm import tqdm #displaying progress bars
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, UpSampling2D
from tensorflow.keras.optimizers import Adam

"""Check the loaded data"""

# Path to the extracted dataset
dataset_path = "/content/CT_Dataset/"

# List files in the dataset
files_in_dataset = os.listdir(dataset_path)
print("Files in dataset directory:", files_in_dataset)

images_path ='/content/CT_Dataset/frames'
masks_path = '/content/CT_Dataset/masks'

#Creating List
images_folder= os.listdir(images_path) #list of filenames of all images
masks_folder= os.listdir(masks_path)

# Check 1: Match the number of files
if len(images_folder) == len(masks_folder):
    print(f"Number of images ({len(images_folder)}) matches number of masks ({len(masks_folder)}).")
else:
    print(f"Mismatch! {len(images_folder)} images and {len(masks_folder)} masks.")

# Check 2: Match filenames to ensure images and corresponding masks have same filename
mismatch_count = 0
for img, mask in zip(images_folder, masks_folder):
    if os.path.splitext(img)[0] != os.path.splitext(mask)[0]:
        print(f"Mismatch: Image {img} does not correspond to Mask {mask}")
        mismatch_count += 1

if mismatch_count == 0:
    print("All filenames match correctly!")
else:
    print(f"Total mismatches: {mismatch_count}.")

"""Check for completely black mask and remove if found (Doesn't contain any information for Segmentation)"""

# Path to the dataset folder
dataset_folder = "/content/CT_Dataset"

# Define paths for masks and images
masks_folder = os.path.join(dataset_folder, "masks")
frames_folder = os.path.join(dataset_folder, "frames")

# Get list of mask files
mask_files = os.listdir(masks_folder)

# Count for removed files
removed_count = 0

# Iterate through each mask file
for mask_file in mask_files:
    mask_path = os.path.join(masks_folder, mask_file)

    # Read the mask as an image
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    # Check if the mask is completely black (all pixels are 0)
    if np.all(mask == 0):
        # Print the message for completely black mask found
        print(f"Completely black mask found: {mask_file}")

        # Corresponding image file path
        image_file = mask_file.replace('mask', 'image')
        image_path = os.path.join(frames_folder, image_file)

        # Remove the mask and image file
        os.remove(mask_path)
        os.remove(image_path)

        # Increment removed count
        removed_count += 1

# After removal, Another check to confirm number of remaining mask and image files are equal
remaining_masks = os.listdir(masks_folder)
remaining_images = os.listdir(frames_folder)

print(f"Number of removed files: {removed_count}")
print(f"Remaining mask files: {len(remaining_masks)}")
print(f"Remaining image files: {len(remaining_images)}")

# Ensure the remaining counts are equal
assert len(remaining_masks) == len(remaining_images), "Mismatch between mask and image counts." #safeguard to check the mentioned statement is true or false.

"""Pre-processing (Gaussian Smoothing and CLAHE Contrast enhancement)"""

def preprocess_image(image_path, apply_smoothing=False):

    # Read image in grayscale
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Apply CLAHE
    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))
    processed_image = clahe.apply(image)

    #Apply Gaussian smoothing ; Important to reduce noise which may be enhanced during contrats enhancement
    if apply_smoothing:
        processed_image = cv2.GaussianBlur(processed_image, (3, 3), 1)

    return processed_image

def preprocess_all_images(frames_folder, output_folder, apply_smoothing=False):

    #Preprocess all images in frames_folder and save to output_folder.

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    image_files = sorted(os.listdir(frames_folder))

    for image_file in image_files:
        image_path = os.path.join(frames_folder, image_file)
        output_path = os.path.join(output_folder, image_file)

        # Preprocess image
        processed_image = preprocess_image(image_path)

        # Save preprocessed image
        cv2.imwrite(output_path, processed_image)

    print(f"All preprocessed images are saved in '{output_folder}'.")

def visualize_results(frames_folder, masks_folder, output_folder, num_samples=3):

   #Visualize random samples of original, preprocessed images, and corresponding masks.

    image_files = sorted(os.listdir(frames_folder))
    mask_files = sorted(os.listdir(masks_folder))
    preprocessed_files = sorted(os.listdir(output_folder))

    # Check to ensure the number of images, masks, and preprocessed files match
    if not (len(image_files) == len(mask_files) == len(preprocessed_files)):
        raise ValueError("Number of images, masks, and preprocessed files do not match.")

    # Randomly select samples
    sample_indices = random.sample(range(len(image_files)), num_samples)

    # Plot num_samples images
    plt.figure(figsize=(15, 5 * num_samples))
    for i, idx in enumerate(sample_indices):
        image_path = os.path.join(frames_folder, image_files[idx])
        mask_path = os.path.join(masks_folder, mask_files[idx])
        preprocessed_path = os.path.join(output_folder, preprocessed_files[idx])

        # Load images and masks
        original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        preprocessed = cv2.imread(preprocessed_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        # Original image
        plt.subplot(num_samples, 3, i * 3 + 1)
        plt.imshow(original, cmap='gray')
        plt.title("Original Image")
        plt.axis('off')

        # Preprocessed image
        plt.subplot(num_samples, 3, i * 3 + 2)
        plt.imshow(preprocessed, cmap='gray')
        plt.title("Preprocessed Image")
        plt.axis('off')

        # Ground truth mask
        plt.subplot(num_samples, 3, i * 3 + 3)
        plt.imshow(mask, cmap='gray')
        plt.title("Ground Truth Mask")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Paths to folders
output_folder = "/content/Filtered_images"      # Folder to save preprocessed images

# Preprocess all images and save them
preprocess_all_images(frames_folder, output_folder, apply_smoothing=True)

# Visualize random samples
visualize_results(frames_folder, masks_folder, output_folder, num_samples=3)

"""Organizing Data into Test and Train Split"""

frames_path = '/content/Filtered_images'
masks_path =   '/content/CT_Dataset/masks' # Assuming all masks are here

# Create train/test directories
train_frames_path = os.path.join(dataset_path, "train", "frames")
train_masks_path = os.path.join(dataset_path, "train", "masks")
test_frames_path = os.path.join(dataset_path, "test", "frames")
test_masks_path = os.path.join(dataset_path, "test", "masks")

os.makedirs(train_frames_path, exist_ok=True)
os.makedirs(train_masks_path, exist_ok=True)
os.makedirs(test_frames_path, exist_ok=True)
os.makedirs(test_masks_path, exist_ok=True)

# Get all frames and masks
frames = sorted(glob(os.path.join(frames_path, "*.png")))
masks = sorted(glob(os.path.join(masks_path, "*.png")))

# Ensure equal numbers of frames and masks
assert len(frames) == len(masks), "Mismatch between frames and masks count!"

# Split into train and test
frames_train, frames_test, masks_train, masks_test = train_test_split(
    frames, masks, test_size=0.2, random_state=42
)

# Move files to train/test directories
for frame, mask in zip(frames_train, masks_train):
    shutil.copy(frame, train_frames_path)
    shutil.copy(mask, train_masks_path)

for frame, mask in zip(frames_test, masks_test):
    shutil.copy(frame, test_frames_path)
    shutil.copy(mask, test_masks_path)

print("Dataset organized successfully!")

"""Displaying some images from Training and Testing Data"""

def display_images_with_masks(frames_path, masks_path, num_images=2, title=""):
    # Get all frames and masks
    frames = sorted(glob(os.path.join(frames_path, "*.png")))
    masks = sorted(glob(os.path.join(masks_path, "*.png")))

    # Select random indices
    indices = random.sample(range(len(frames)), num_images)

    # Display images and masks
    for idx in indices:
        # Read the image and mask
        frame = cv2.imread(frames[idx], cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(masks[idx], cv2.IMREAD_GRAYSCALE)

        # Create overlay (optional for better visualization)
        overlay = cv2.addWeighted(frame, 0.7, mask, 0.3, 0)

        # Plot
        plt.figure(figsize=(12, 4))

        # Display original frame
        plt.subplot(1, 3, 1)
        plt.imshow(frame, cmap='gray')
        plt.title("Original Image")
        plt.axis('off')

        # Display mask
        plt.subplot(1, 3, 2)
        plt.imshow(mask, cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Display overlay

        # Convert mask to colored (green channel will be used for visibility)
        mask_colored = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
        mask_colored[:, :, 1] = 0  # Set green channel to 0
        mask_colored[:, :, 2] = 0  # Set blue channel to 0

        # Stack the image and mask for visualization (overlay effect)
        overlay = cv2.addWeighted(cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR), 0.5, mask_colored, 0.5, 0)

        plt.subplot(1, 3, 3)
        plt.imshow(overlay, cmap='gray')
        plt.title("Overlay")
        plt.axis('off')

        plt.suptitle(title, fontsize=16)
        plt.show()

# Example Usage:
print("                                    IMAGES FROM TRAINING DATA")
display_images_with_masks(train_frames_path, train_masks_path, num_images=2)

print("                                     IMAGES FROM TESTING DATA")
display_images_with_masks(test_frames_path, test_masks_path, num_images=2)

def load_data_paths(path):

    #Load paths to frames and masks.
    train_path = os.path.join(path, "train")
    test_path = os.path.join(path, "test")

    train_frames = sorted(glob(os.path.join(train_path, "frames", "*.png")))
    train_masks = sorted(glob(os.path.join(train_path, "masks", "*.png")))

    test_frames = sorted(glob(os.path.join(test_path, "frames", "*.png")))
    test_masks = sorted(glob(os.path.join(test_path, "masks", "*.png")))

    return (train_frames, train_masks), (test_frames, test_masks)

def augment_and_save(image_paths, mask_paths, output_dir, augment=True, img_size=(256, 256), lesion_threshold=0.01):
    #Perform augmentation and save the results to disk.
    os.makedirs(output_dir, exist_ok=True)
    frame_dir = os.path.join(output_dir, "frames")
    mask_dir = os.path.join(output_dir, "masks")
    os.makedirs(frame_dir, exist_ok=True)
    os.makedirs(mask_dir, exist_ok=True)

    count = 0
    for image_path, mask_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths)):
        # Read and preprocess image and mask
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        image = cv2.resize(image, img_size) / 255.0
        mask = cv2.resize(mask, img_size) / 255.0

        # Calculate lesion size as the percentage of non-zero pixels
        lesion_size = np.sum(mask > 0) / mask.size #mask.size will be giving width and height means total no of pixels in image ocntaining masks

        if augment and lesion_size <= lesion_threshold:  # Augment only small lesions
            # Apply augmentations
            aug = HorizontalFlip(p=1.0)
            augmented = aug(image=image, mask=mask)
            cv2.imwrite(os.path.join(frame_dir, f"{count}_hflip.png"), (augmented['image'] * 255).astype(np.uint8))
            cv2.imwrite(os.path.join(mask_dir, f"{count}_hflip.png"), (augmented['mask'] * 255).astype(np.uint8))
            count += 1

            aug = VerticalFlip(p=1.0)
            augmented = aug(image=image, mask=mask)
            cv2.imwrite(os.path.join(frame_dir, f"{count}_vflip.png"), (augmented['image'] * 255).astype(np.uint8))
            cv2.imwrite(os.path.join(mask_dir, f"{count}_vflip.png"), (augmented['mask'] * 255).astype(np.uint8))
            count += 1

            aug = Rotate(limit=45, p=1.0)
            augmented = aug(image=image, mask=mask)
            cv2.imwrite(os.path.join(frame_dir, f"{count}_rotate.png"), (augmented['image'] * 255).astype(np.uint8))
            cv2.imwrite(os.path.join(mask_dir, f"{count}_rotate.png"), (augmented['mask'] * 255).astype(np.uint8))
            count += 1

        # Save original data
        cv2.imwrite(os.path.join(frame_dir, f"{count}_orig.png"), (image * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(mask_dir, f"{count}_orig.png"), (mask * 255).astype(np.uint8))
        count += 1

    print(f"Saved {count} images and masks to {output_dir}")

if __name__ == "__main__":
    dataset_path = "/content/CT_Dataset"  # Change this to your dataset path
    output_train_dir = "/content/augmented_train"
    output_test_dir = "/content/augmented_test"

    # Load dataset paths
    (train_frames, train_masks), (test_frames, test_masks) = load_data_paths(dataset_path)

    print(f"Original train images: {len(train_frames)}, Original train masks: {len(train_masks)}")
    print(f"Original test images: {len(test_frames)}, Original test masks: {len(test_masks)}")

    # Define lesion size threshold (e.g., 1% of the image size)
    lesion_threshold = 0.01

    # Augment and save training data
    augment_and_save(train_frames, train_masks, output_train_dir, augment=True, lesion_threshold=lesion_threshold)

    # Augment and save test data (no augmentation)
    augment_and_save(test_frames, test_masks, output_test_dir, augment=False)

"""Display Augmented Images"""

def overlay_mask(image, mask):
    #Overlay the mask on the image with red color.

    image = (image * 255).astype(np.uint8)
    mask = (mask * 255).astype(np.uint8)

    # Create a 3-channel (RGB) version of the image
    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)

    # Create a red mask (RGB) by setting the red channel to the mask intensity
    red_mask = np.zeros_like(image_rgb)
    red_mask[mask > 0] = [255, 0, 0]  # Red color for mask areas

    # Overlay the red mask on the image (with transparency)
    overlayed_image = cv2.addWeighted(image_rgb, 1, red_mask, 0.5, 0)

    return overlayed_image

def display_augmented_images(image_dir, mask_dir, num_images=3):

    #Display original, horizontally flipped, vertically flipped, and rotated augmented images with their corresponding masks overlayed in red.

    # List image and mask files in the provided directories
    image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('_orig.png')])  # Filter original images
    mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('_orig.png')])  # Filter original masks

    # Check if there are enough images to display
    if len(image_paths) == 0 or len(mask_paths) == 0:
        print("No images or masks found. Exiting.")
        return

    # If there are fewer than num_images, adjust num_images to match the available count
    num_images = min(num_images, len(image_paths), len(mask_paths))

    fig, axes = plt.subplots(num_images, 4, figsize=(15, 5 * num_images))

    for i in range(num_images):
        # Load the original image and mask
        image = cv2.imread(image_paths[i], cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_paths[i], cv2.IMREAD_GRAYSCALE)

        # Check if the image and mask are loaded correctly
        if image is None or mask is None:
            print(f"Error: Could not load image or mask at index {i}. Skipping.")
            continue

        # Normalize the image and mask to the range [0, 1]
        image = image / 255.0
        mask = mask / 255.0

        # Apply augmentations: horizontally flip, vertically flip, rotate
        hflip_image = cv2.flip(image, 1)
        vflip_image = cv2.flip(image, 0)
        rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)

        hflip_mask = cv2.flip(mask, 1)
        vflip_mask = cv2.flip(mask, 0)
        rotated_mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE)

        # Overlay masks in red over the images
        original_overlay = overlay_mask(image, mask)
        hflip_overlay = overlay_mask(hflip_image, hflip_mask)
        vflip_overlay = overlay_mask(vflip_image, vflip_mask)
        rotated_overlay = overlay_mask(rotated_image, rotated_mask)

        # Plot the original image, augmented images, and corresponding masks with overlay
        axes[i, 0].imshow(original_overlay)
        axes[i, 0].set_title(f'Original Image {i+1}')
        axes[i, 0].axis('off')

        axes[i, 1].imshow(hflip_overlay)
        axes[i, 1].set_title(f'Horizontal Flip {i+1}')
        axes[i, 1].axis('off')

        axes[i, 2].imshow(vflip_overlay)
        axes[i, 2].set_title(f'Vertical Flip {i+1}')
        axes[i, 2].axis('off')

        axes[i, 3].imshow(rotated_overlay)
        axes[i, 3].set_title(f'Rotated Image {i+1}')
        axes[i, 3].axis('off')

    plt.tight_layout()
    plt.show()

# Example usage
image_dir = '/content/augmented_test/frames'  # Path to your augmented frames folder
mask_dir = '/content/augmented_test/masks'    # Path to your augmented masks folder

display_augmented_images(image_dir, mask_dir)

"""Normalized frames and masks are loaded + dimension is added"""

IMG_SIZE=(256,256)

# Load Data Function
def load_augmented_data(data_dir):
    #Load augmented images and masks.
    frame_dir = os.path.join(data_dir, "frames")
    mask_dir = os.path.join(data_dir, "masks")

    frames = sorted(glob(os.path.join(frame_dir, "*.png")))
    masks = sorted(glob(os.path.join(mask_dir, "*.png")))

    images = []
    labels = []

    for frame_path, mask_path in zip(frames, masks):
        # Load and normalize frame and mask
        frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        frame = cv2.resize(frame, IMG_SIZE) / 255.0  # Normalize to [0, 1]
        mask = cv2.resize(mask, IMG_SIZE) / 255.0  # Normalize to [0, 1]

        images.append(frame)
        labels.append(mask)

    images = np.expand_dims(np.array(images), axis=-1)
    labels = np.expand_dims(np.array(labels), axis=-1)

    return images, labels

"""Loading splitted data and printing to check shape"""

# Load Data
train_data_dir = "/content/augmented_train"
test_data_dir = "/content/augmented_test"

x_train, y_train = load_augmented_data(train_data_dir)
x_test, y_test = load_augmented_data(test_data_dir)

# Split training data into training and validation
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

print(f"x_train: {x_train.shape}, y_train: {y_train.shape}")
print(f"x_val: {x_val.shape}, y_val: {y_val.shape}")
print(f"x_test: {x_test.shape}, y_test: {y_test.shape}")

"""Defining Traditional UNet (1st Model)"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate
from tensorflow.keras.optimizers import Adam

# U-Net model definition
def unet_model(input_size=(256, 256, 1)):
    inputs = Input(input_size)

    # Contracting Path
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = MaxPooling2D((2, 2))(c1)

    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = MaxPooling2D((2, 2))(c2)

    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = MaxPooling2D((2, 2))(c3)

    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
    p4 = MaxPooling2D((2, 2))(c4)

    # Bottleneck
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)

    # Expansive Path
    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = concatenate([u6, c4])
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)

    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = concatenate([u7, c3])
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)

    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)

    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)
    u9 = concatenate([u9, c1])
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)

    model = Model(inputs, outputs)
    return model

# Instantiate and compile the model
model = unet_model(input_size=(256, 256, 1))

# Model summary
model.summary()

"""Definition of Dice-coeffecient function"""

import tensorflow as tf

# Dice coefficient
def dice_coefficient(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)  # Ensure both tensors are float32
    y_pred = tf.cast(y_pred, tf.float32)

    y_true_f = tf.keras.backend.flatten(y_true)
    y_pred_f = tf.keras.backend.flatten(y_pred)
    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)

    return (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)

"""Definig IOU Score"""

def iou_score(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)  # Ensure both tensors are float32
    y_pred = tf.cast(y_pred, tf.float32)

    y_true_f = tf.keras.backend.flatten(y_true)
    y_pred_f = tf.keras.backend.flatten(y_pred)
    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
    union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection
    iou = (intersection + 1) / (union + 1)
    return iou

"""Compiling Model"""

# Define the optimizer (e.g., Adam optimizer)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

# Define the loss function (Binary Crossentropy)
loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)

# Assuming `Model` is already defined
model.compile(optimizer=optimizer,
              loss=loss_fn,
              metrics=['accuracy',dice_coefficient,iou_score])

"""Training Model"""

# Train Model
history = model.fit(x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=8,
    epochs=50,
    verbose=1)

"""Plot of Matrices"""

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 8))

# Accuracy plot
plt.subplot(2, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='green')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Loss plot
plt.subplot(2, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', color='red')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

# Dice coefficient plot
plt.subplot(2, 2, 3)
plt.plot(history.history['dice_coefficient'], label='Training Dice Coefficient', color='purple')
plt.plot(history.history['val_dice_coefficient'], label='Validation Dice Coefficient', color='brown')
plt.title('Dice Coefficient')
plt.xlabel('Epoch')
plt.ylabel('Dice Coefficient')
plt.legend(loc='lower right')

plt.tight_layout()
plt.show()

"""Evaluation of Model"""

# Evaluate the model on the test data
test_loss, test_accuracy, test_dice_coefficient,test_iou_score= model.evaluate(x_test, y_test)

print(f"Test loss: {test_loss}")
print(f"Test accuracy: {test_accuracy}")
print(f"Test-Dice Coffecient: {test_dice_coefficient}")
print(f"Test-IOU Score: {test_iou_score}")

# Generate predictions
predictions = model.predict(x_test)

# Convert predictions to binary masks (if necessary)
predicted_masks = (predictions > 0.5).astype(np.uint8)

"""Display some test images for visual analysis of Model performance"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

def visualize_predictions(model, x_test, y_test, num_images=10):

    # Make predictions on the test set
    predictions = model.predict(x_test)

    # Convert predictions to binary values (0 or 1)
    predictions = tf.round(predictions)  # Rounding the predictions

    # Loop through the specified number of images
    for i in range(num_images):
        plt.figure(figsize=(12, 6))

        # Display the original image
        plt.subplot(1, 3, 1)
        plt.imshow(x_test[i].reshape(x_test.shape[1], x_test.shape[2]), cmap='gray')
        plt.title("Original Image")
        plt.axis('off')

        # Display the ground truth mask
        plt.subplot(1, 3, 2)
        ground_truth_mask = y_test[i].reshape(y_test.shape[1], y_test.shape[2])
        plt.imshow(ground_truth_mask, cmap='gray')
        plt.title("Ground Truth Mask")
        plt.axis('off')

        # Display the predicted mask
        plt.subplot(1, 3, 3)
        pred_mask = predictions[i].numpy().reshape(x_test.shape[1], x_test.shape[2])
        plt.imshow(pred_mask, cmap='gray')
        plt.title("Predicted Mask")
        plt.axis('off')

        plt.show()

# Example usage:
# Assuming x_test and y_test are pre-loaded and model is trained
visualize_predictions(model, x_test, y_test, num_images=10)

"""RES-NET (2nd Model)"""

from tensorflow.keras.layers import Add, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Input
from tensorflow.keras.models import Model

# Residual U-Net Model Definition
def res_unet_model(input_size=(256, 256, 1)):
    inputs = Input(input_size)

    # Contracting Path (Residual Blocks)
    c1 = Conv2D(64, (3, 3), activation=None, padding='same')(inputs)
    c1 = Conv2D(64, (3, 3), activation=None, padding='same')(c1)
    shortcut1 = Conv2D(64, (1, 1), padding='same')(inputs)  # Adjust input channels
    r1 = Add()([c1, shortcut1])  # Residual Connection
    p1 = MaxPooling2D((2, 2))(r1)

    c2 = Conv2D(128, (3, 3), activation=None, padding='same')(p1)
    c2 = Conv2D(128, (3, 3), activation=None, padding='same')(c2)
    shortcut2 = Conv2D(128, (1, 1), padding='same')(p1)  # Adjust channels
    r2 = Add()([c2, shortcut2])  # Residual Connection
    p2 = MaxPooling2D((2, 2))(r2)

    c3 = Conv2D(256, (3, 3), activation=None, padding='same')(p2)
    c3 = Conv2D(256, (3, 3), activation=None, padding='same')(c3)
    shortcut3 = Conv2D(256, (1, 1), padding='same')(p2)  # Adjust channels
    r3 = Add()([c3, shortcut3])  # Residual Connection
    p3 = MaxPooling2D((2, 2))(r3)

    c4 = Conv2D(512, (3, 3), activation=None, padding='same')(p3)
    c4 = Conv2D(512, (3, 3), activation=None, padding='same')(c4)
    shortcut4 = Conv2D(512, (1, 1), padding='same')(p3)  # Adjust channels
    r4 = Add()([c4, shortcut4])  # Residual Connection
    p4 = MaxPooling2D((2, 2))(r4)

    # Bottleneck
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)

    # Expansive Path (Residual Blocks)
    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = concatenate([u6, r4])
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)

    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = concatenate([u7, r3])
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)

    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = concatenate([u8, r2])
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)

    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)
    u9 = concatenate([u9, r1])
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)

    model = Model(inputs, outputs)
    return model

# Instantiate and compile the RES U-Net model
model = res_unet_model(input_size=(256, 256, 1))

# Model summary
model.summary()

model.save('Res U-Net.h5')

"""Attention Unet (3rd Model)"""

from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Activation, Multiply, Add
from tensorflow.keras.models import Model

# Attention Gate Definition
def attention_gate(skip_connection, gating_signal, inter_channels):
    # 1x1 Convolution for skip connection
    theta_x = Conv2D(inter_channels, (1, 1), strides=(1, 1), padding='same')(skip_connection)
    # 1x1 Convolution for gating signal
    phi_g = Conv2D(inter_channels, (1, 1), strides=(1, 1), padding='same')(gating_signal)
    # Add the two
    add = Add()([theta_x, phi_g])
    # ReLU activation
    act = Activation('relu')(add)
    # 1x1 Convolution to generate attention coefficients
    psi = Conv2D(1, (1, 1), strides=(1, 1), padding='same')(act)
    psi = Activation('sigmoid')(psi)
    # Multiply attention coefficients with the skip connection
    attn_out = Multiply()([skip_connection, psi])
    return attn_out

# Attention U-Net Model Definition
def attention_unet(input_size=(256, 256, 1)):
    inputs = Input(input_size)

    # Encoder
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = MaxPooling2D((2, 2))(c1)

    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = MaxPooling2D((2, 2))(c2)

    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = MaxPooling2D((2, 2))(c3)

    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
    p4 = MaxPooling2D((2, 2))(c4)

    # Bottleneck
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)

    # Decoder
    u6 = UpSampling2D((2, 2))(c5)
    attn1 = attention_gate(c4, u6, 512)
    u6 = concatenate([u6, attn1])
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)

    u7 = UpSampling2D((2, 2))(c6)
    attn2 = attention_gate(c3, u7, 256)
    u7 = concatenate([u7, attn2])
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)

    u8 = UpSampling2D((2, 2))(c7)
    attn3 = attention_gate(c2, u8, 128)
    u8 = concatenate([u8, attn3])
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)

    u9 = UpSampling2D((2, 2))(c8)
    attn4 = attention_gate(c1, u9, 64)
    u9 = concatenate([u9, attn4])
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)

    model = Model(inputs, outputs)
    return model

# Instantiate and compile the Attention U-Net model
model = attention_unet(input_size=(256, 256, 1))


# Model summary
model.summary()

model.save('Attention U-Net.h5')

import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Load the trained Attention U-Net model
model = tf.keras.models.load_model("Attention U-Net.h5", compile=False)  # Load without compiling

# Define layer names where attention maps are generated
attention_layer_names = ["add_1", "add_2", "add_3", "add_4"]  # Update with actual layer names from model.summary()

# Create a new model that outputs segmentation + attention maps
attention_model = tf.keras.Model(inputs=model.input,
                                 outputs=[model.output] + [model.get_layer(name).output for name in attention_layer_names])

# Function to preprocess input image
def preprocess_image(image_path, target_size=(256, 256)):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size, color_mode="grayscale")
    img = tf.keras.preprocessing.image.img_to_array(img)
    img = img / 255.0  # Normalize
    img = np.expand_dims(img, axis=0)  # Add batch dimension
    return img

# Function to overlay attention maps on the original image
def overlay_attention(image, attention_map):
    attention_map = cv2.resize(attention_map, (image.shape[1], image.shape[0]))  # Resize attention map
    attention_map = (attention_map - np.min(attention_map)) / (np.max(attention_map) - np.min(attention_map))  # Normalize
    attention_map = (attention_map * 255).astype(np.uint8)

    heatmap = cv2.applyColorMap(attention_map, cv2.COLORMAP_JET)  # Apply heatmap
    overlay = cv2.addWeighted(image, 0.6, heatmap, 0.4, 0)  # Blend with original image

    return overlay

# Load an image and get model predictions
image_path = "path_to_ct_scan.png"  # Update with your test image
image = preprocess_image(image_path)
outputs = attention_model.predict(image)

segmentation_mask = outputs[0][0]  # Final segmentation output
attention_maps = outputs[1:]  # Extract attention maps

# Convert grayscale image to RGB for visualization
image_vis = cv2.imread(image_path)
image_vis = cv2.cvtColor(image_vis, cv2.COLOR_BGR2RGB)

# Plot segmentation and attention maps
plt.figure(figsize=(12, 6))

plt.subplot(1, len(attention_maps) + 2, 1)
plt.imshow(image_vis, cmap="gray")
plt.title("Original Image")
plt.axis("off")

plt.subplot(1, len(attention_maps) + 2, 2)
plt.imshow(segmentation_mask.squeeze(), cmap="gray")
plt.title("Segmentation Mask")
plt.axis("off")

# Loop through attention maps and visualize them
for i, attn_map in enumerate(attention_maps):
    overlay = overlay_attention(image_vis, attn_map[0].squeeze())  # Get first batch sample
    plt.subplot(1, len(attention_maps) + 2, i + 3)
    plt.imshow(overlay)
    plt.title(f"Attention Map {i+1}")
    plt.axis("off")

plt.show()